{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3185c51f-744e-477e-b208-0c0f9a762397",
   "metadata": {},
   "source": [
    "# Exploring Open-Source LLMs: A Comparative Analysis of LLaMA, Mistral, and Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949758a-4e2e-432c-b64d-e40fc44890b3",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab is a comparitive analysis of LLaMA 3.1: 8B, Mistral: 7B, and Phi-3.5B across three tasks: content writing, code generation, and text summarization. Participants will analyze the strengths and weaknesses of each model by comparing outputs in terms of coherence, accuracy, and creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9626d3e-c79d-4bce-95eb-a3217a457efa",
   "metadata": {},
   "source": [
    "## Lab Objectives:\n",
    "\n",
    "### After completing this lab, participants will be able to:\n",
    "\n",
    "- Evaluate the performance of open-source LLMs (LLaMA 3.1:8B, Mistral:7B, and Phi-3:5B) in tasks like text summarization, code generation, and content writing.\n",
    "\n",
    "- Compare and contrast model outputs to determine relative strengths, weaknesses, and suitable applications.\n",
    "\n",
    "- Identify potential use-cases and practical implications of each model for real-world scenarios.\n",
    "\n",
    "- Understand the trade-offs between model size, resource consumption, inference speed, and task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6dd08-c34d-47c9-9a22-75315b68c627",
   "metadata": {},
   "source": [
    "## Lab Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc998470-cd39-4069-b506-7a6999b14502",
   "metadata": {},
   "source": [
    "The participant makes a request to the Ollama server running on the DL380a. The request contains the Prompt to the LLM. The LLM hosted on the server returns a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74797-9e01-490c-94ec-fa14f0e71025",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"flow.png\" alt=\"flow\" width=\"700\" height=\"450\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98506bdb-c914-4514-8f91-656419983116",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acede7a9-cf4e-42cc-a65f-99aae95df3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bc406-2578-40f1-a5a6-fdcac4b61b76",
   "metadata": {},
   "source": [
    "## Content Writing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2eab63-44a0-41f7-9017-c7062b8d089f",
   "metadata": {},
   "source": [
    "### Mistral:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0530d-f0ae-4842-9336-d15405f429de",
   "metadata": {},
   "source": [
    "Mistral 7B is a 7.3 billion parameter model. It is one of the most powerful language models of its size. Mistral performs near the level of larger models like GPT-3.5. All while being efficient in terms of computational efficiency and memory usage. \n",
    "\n",
    "Mistral is available on Ollama and can be used for inferencing. Let us test Mistral's ability to write content. We use langchain to provide a prompt to the LLM and ask it to write a blog post on Large Language Models. Langchain is a framework that simplifies working with LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77c450-9352-4708-9018-b37ea6fff580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")   #loading the mistral:7b (latest) model from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60eafc-8ac7-4a1e-b0a1-1cc4ad793691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the prompt template to the LLM, {context} can be formatted with a user query\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "#the content for the LLM to write blog post on \n",
    "context = \"Large Language Models\"\n",
    "\n",
    "#building the chain\n",
    "chain = prompt | model_mistral | StrOutputParser()\n",
    "\n",
    "#invoking the chain \n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "#display the response in markdown format\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad401b6-67ca-4686-97dd-fb57cdca2bfb",
   "metadata": {},
   "source": [
    "We can see that mistral generated a pretty good response. One thing we could notice is that the formatting is not upto the mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a9456-0281-4ae0-a3be-c556f2ee840b",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b\n",
    "\n",
    "LLaMA 3.1: 8B is a highly advanced language model developed by Meta, designed to provide state-of-the-art performance with just 8 billion parameters. This model is bigger than the previous Mistral:7b model we tested. Let us now test how LLaMA3.1:8b generates the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98b472-e35e-4918-a79d-bab69058fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")  #loading llama3.1:8b from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4e773-8f9e-4b80-bcc8-3203092e43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "context = \"Large Language Models\"\n",
    "\n",
    "chain = prompt | model_llama | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bb86b-ddaa-49d2-acf6-d25345f727d7",
   "metadata": {},
   "source": [
    "The model generates a well formatted and structured response. The output looks better that the one generated by Mistral:7b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708478c-1488-4370-bc01-5ffdb7b807f8",
   "metadata": {},
   "source": [
    "### Phi3.5\n",
    "\n",
    "Let us now test a lightweight language model. It has only 3.8 billion parameters. But it is known to overtake similar or even larger sized models. It is an open-source model developed by microsoft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11f67a-8928-4649-896c-5d77c0eafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\") #loading phi3.5:latest form ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3dc37-96b5-4f58-9e14-df16e6dc2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "context = \"Large Language Models\"\n",
    "\n",
    "chain = prompt | model_phi | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d92bd4-7a9c-4df8-a78c-337a0c6607f1",
   "metadata": {},
   "source": [
    "We immediately notice the difference in response generated by phi3.5 when compared to mistral and llama. This is primarily beacuse phi3.5 is a smaller model that the other two. The model generates complicated responses that doesn't look very natural. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1a42f-d21a-46b3-8a8b-70820d5a17dd",
   "metadata": {},
   "source": [
    "## Code Generation \n",
    "\n",
    "Let us test the code generation capabilities of all these models. We give a prompt to each of these models to generate a python function, and then analyze the response of each model. Feel free to edit the prompt and make the models generate other responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f4ba9-4a9e-469f-91ec-81eb1bc5d1b6",
   "metadata": {},
   "source": [
    "### Mistral\n",
    "\n",
    "Mistral:7b is really good at coding tasks. I even comes near to CodeLlama 7b at code generation tasks while being equally good at English language. Let us put Mistral's coding abilities to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eecba6-b137-4848-9534-0fc4d68a8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffc2b1-ee7a-44e1-8141-555e92c8aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_mistral.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25513aaa-fa1d-4c4f-bc82-69c8a6009e24",
   "metadata": {},
   "source": [
    "It generated a pretty good response. But we can notice that it did not provide a function description, return type and the argument definitions. Apart from that, the code is straight forward and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b23257-ace3-4ad1-8b07-b8613822b872",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b \n",
    "\n",
    "LLaMA3.1:8b is a really good LLM for coding tasks. It outperforms most of the models of its size and even comes closer to some bigger models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e27a1-892b-4e4f-b4f5-c23a0eded2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ae78f-2814-47d7-ab02-e5bc40b9ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_llama.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a260262-9672-4a3f-a571-191c707b959e",
   "metadata": {},
   "source": [
    "The response generated by LLaMa is really good. It provided all the function description and argument definitions. It also generated the shortcomings of computing factorials using the recursive approach. The generated response is self explanatory, anybody reading it can understand what the code is about. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ec069-98cc-4066-8eb8-56b36869189e",
   "metadata": {},
   "source": [
    "### Phi3.5\n",
    "\n",
    "Let us test the coding abilities of a really light-weight LLM and see how it performs against larger LLMs like Mistral & LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafb752-9946-4fa7-be31-996094696af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611fca-e8ee-478f-9530-016a7117eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_phi.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da03660-d0e3-49ab-8d76-ae6e87a86ee1",
   "metadata": {},
   "source": [
    "The code and explanations that follow looks good. However, including the base case of `n == 1` is missing here. Although this doesn't change the output, it might result in an additional recursion call, unnecessarily increasing recursion depth. So the response by Mistral or LLaMA is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc33ab-11a3-4330-95ac-472d435f1120",
   "metadata": {},
   "source": [
    "## Text Summarization \n",
    "\n",
    "Large Language Models (LLMs) are highly effective for text summarization as they can grasp context and extract key information across lengthy texts. They leverage extensive training on diverse data to generate concise summaries while retaining the original meaning and essential details. LLMs handle various summarization styles, from extractive (directly pulling important sentences) to abstractive (generating novel sentences). This adaptability makes them valuable for applications across industries, from media and research to customer support and legal fields, improving efficiency in processing vast amounts of information. \n",
    "\n",
    "We provide a paragraph on HPE Proliant servers to each of these LLMS and ask them to summarize it in 2 short sentences. We can then analyze each outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e22564-8c1e-489f-aa5e-afd8d3a93498",
   "metadata": {},
   "source": [
    "### Mistral:7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9523f04-f91f-4064-827b-afdfc506644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1a7da-1d69-4646-b7e6-33131b0189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_mistral | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8c145-05fa-40c8-8600-11fe2fb05196",
   "metadata": {},
   "source": [
    "Mistral captured all the important details in the original paragraph. But, it provided two large sentences. It wasn't able to provide a short summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072dc27-b715-41a9-b61e-1f3fbe74f0f2",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2323f-dddc-4860-88ed-35ff7a630e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521ecec-4f83-4c31-aa43-5c0a8d33420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_llama | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194c96-3e09-474b-845d-3d8f58c033e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533dc3c-644f-4ab0-a26c-1f7606b0d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_phi | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fec9c2-514e-4234-b18d-829f8edb345b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0445f-e8f2-4fc2-9612-94b43b673719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
