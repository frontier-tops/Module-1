{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3185c51f-744e-477e-b208-0c0f9a762397",
   "metadata": {},
   "source": [
    "# Exploring Open-Source LLMs: A Comparative Analysis of LLaMA, Mistral, and Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949758a-4e2e-432c-b64d-e40fc44890b3",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab is a comparitive analysis of LLaMA 3.1: 8B, Mistral: 7B, and Phi-3.5B across three tasks: content writing, code generation, and text summarization. Participants will analyze the strengths and weaknesses of each model by comparing outputs in terms of coherence, accuracy, and creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9626d3e-c79d-4bce-95eb-a3217a457efa",
   "metadata": {},
   "source": [
    "## Lab Objectives:\n",
    "\n",
    "### After completing this lab, participants will be able to:\n",
    "\n",
    "- Evaluate the performance of open-source LLMs (LLaMA 3.1:8B, Mistral:7B, and Phi-3:5B) in tasks like text summarization, code generation, and content writing.\n",
    "\n",
    "- Compare and contrast model outputs to determine relative strengths, weaknesses, and suitable applications.\n",
    "\n",
    "- Identify potential use-cases and practical implications of each model for real-world scenarios.\n",
    "\n",
    "- Understand the trade-offs between model size, resource consumption, inference speed, and task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6dd08-c34d-47c9-9a22-75315b68c627",
   "metadata": {},
   "source": [
    "## Lab Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc998470-cd39-4069-b506-7a6999b14502",
   "metadata": {},
   "source": [
    "The participant makes a request to the Ollama server running on the DL380a. The request contains the Prompt to the LLM. The LLM hosted on the server returns a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74797-9e01-490c-94ec-fa14f0e71025",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"flow.png\" alt=\"flow\" width=\"700\" height=\"450\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98506bdb-c914-4514-8f91-656419983116",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acede7a9-cf4e-42cc-a65f-99aae95df3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bc406-2578-40f1-a5a6-fdcac4b61b76",
   "metadata": {},
   "source": [
    "## Content Writing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2eab63-44a0-41f7-9017-c7062b8d089f",
   "metadata": {},
   "source": [
    "### Mistral:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0530d-f0ae-4842-9336-d15405f429de",
   "metadata": {},
   "source": [
    "Mistral 7B is a 7.3 billion parameter model. It is one of the most powerful language models of its size. Mistral performs near the level of larger models like GPT-3.5. All while being efficient in terms of computational efficiency and memory usage. \n",
    "\n",
    "Mistral is available on Ollama and can be used for inferencing. Let us test Mistral's ability to write content. We use langchain to provide a prompt to the LLM and ask it to write a blog post on Large Language Models. Langchain is a framework that simplifies working with LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae77c450-9352-4708-9018-b37ea6fff580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")   #loading the mistral:7b (latest) model from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b60eafc-8ac7-4a1e-b0a1-1cc4ad793691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Title: Harnessing the Power of Large Language Models\n",
       "\n",
       "In the rapidly evolving landscape of technology, large language models are making significant strides and leaving an indelible mark. These sophisticated AI systems are not only transforming how we interact with digital interfaces but also reshaping various sectors, from education to business.\n",
       "\n",
       "Large language models (LLMs) are artificial intelligence systems capable of understanding and generating human-like text based on the input they receive. They learn from vast amounts of data, allowing them to adapt to a wide range of conversational styles, topics, and even contexts. This versatility makes LLMs invaluable tools for numerous applications.\n",
       "\n",
       "One of the most intriguing aspects of large language models is their potential to augment human capabilities. For instance, they can assist in content creation by generating ideas, drafting articles, or even composing poetry. In the realm of education, LLMs can help students learn more effectively by providing personalized explanations and answers to queries.\n",
       "\n",
       "Moreover, businesses are harnessing the power of LLMs for customer service, marketing, and content generation. By integrating LLMs into their systems, companies can offer 24/7 support, create engaging marketing copy, and generate valuable insights from vast amounts of data, thereby improving decision-making processes.\n",
       "\n",
       "However, it's essential to acknowledge the challenges associated with large language models. One critical concern is ensuring these AI systems maintain high standards of ethics and fairness. As they learn from data, there's a risk that biases could be perpetuated. To address this issue, efforts are being made to improve transparency in how LLMs learn and operate, ultimately leading to more equitable outcomes.\n",
       "\n",
       "Another challenge lies in striking a balance between the model's ability to generate coherent, contextually relevant responses and the risk of generating misinformation. While large language models can provide accurate information most of the time, it's crucial to verify the facts they present, especially when dealing with sensitive or critical topics.\n",
       "\n",
       "As we continue to explore the possibilities offered by large language models, it's essential to approach their development and application with a keen eye towards both their benefits and potential pitfalls. By embracing this technology responsibly and thoughtfully, we can usher in an era of unprecedented efficiency and innovation while minimizing risks and maximizing opportunities for growth.\n",
       "\n",
       "In conclusion, large language models represent a significant leap forward in AI technology. Their ability to understand and generate human-like text makes them versatile tools that can reshape various sectors and augment human capabilities. As we navigate this exciting new frontier, it's crucial to maintain an ethical and responsible approach to their development and use, ensuring the benefits of large language models are accessible to all while minimizing potential risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the prompt template to the LLM, {context} can be formatted with a user query\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "#the content for the LLM to write blog post on \n",
    "context = \"Large Language Models\"\n",
    "\n",
    "#building the chain\n",
    "chain = prompt | model_mistral | StrOutputParser()\n",
    "\n",
    "#invoking the chain \n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "#display the response in markdown format\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad401b6-67ca-4686-97dd-fb57cdca2bfb",
   "metadata": {},
   "source": [
    "We can see that mistral generated a pretty good response. One thing we could notice is that the formatting is not upto the mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a9456-0281-4ae0-a3be-c556f2ee840b",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b\n",
    "\n",
    "LLaMA 3.1: 8B is a highly advanced language model developed by Meta, designed to provide state-of-the-art performance with just 8 billion parameters. This model is bigger than the previous Mistral:7b model we tested. Let us now test how LLaMA3.1:8b generates the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec98b472-e35e-4918-a79d-bab69058fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")  #loading llama3.1:8b from ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79a4e773-8f9e-4b80-bcc8-3203092e43f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**The Rise of Large Language Models: Revolutionizing Human-Computer Interaction**\n",
       "\n",
       "In recent years, there has been a significant surge in the development and deployment of large language models (LLMs). These advanced algorithms have the ability to process and generate human-like text with unprecedented accuracy, opening up new possibilities for human-computer interaction. In this blog post, we'll delve into the world of LLMs, exploring their capabilities, applications, and implications.\n",
       "\n",
       "**What are Large Language Models?**\n",
       "\n",
       "Large language models are a type of artificial intelligence (AI) algorithm that has been trained on massive datasets to learn patterns in language. These models use complex neural networks to generate text based on input prompts or sequences. Unlike traditional natural language processing (NLP) systems, LLMs can understand context, nuances, and even subtleties of human communication.\n",
       "\n",
       "**How do Large Language Models Work?**\n",
       "\n",
       "LLMs operate by processing vast amounts of data through a process called deep learning. This involves multiple layers of neural networks that allow the model to learn intricate relationships between words, phrases, and sentences. The more training data an LLM receives, the better it becomes at recognizing patterns and making informed predictions.\n",
       "\n",
       "**Applications of Large Language Models**\n",
       "\n",
       "The potential applications of LLMs are vast and varied:\n",
       "\n",
       "1. **Virtual Assistants**: LLMs power popular virtual assistants like Siri, Alexa, and Google Assistant, enabling users to interact with devices using natural language.\n",
       "2. **Chatbots**: These models facilitate customer service, helping businesses provide 24/7 support to customers through automated conversations.\n",
       "3. **Content Generation**: LLMs can generate high-quality content, such as articles, blog posts, and even entire books, at an unprecedented scale and speed.\n",
       "4. **Language Translation**: These models enable seamless language translation, bridging cultural divides and facilitating global communication.\n",
       "\n",
       "**Benefits of Large Language Models**\n",
       "\n",
       "The benefits of LLMs are numerous:\n",
       "\n",
       "1. **Increased Efficiency**: Automated tasks, such as data entry and content creation, can save businesses significant time and resources.\n",
       "2. **Improved User Experience**: Virtual assistants and chatbots provide users with instant answers to queries, enhancing customer satisfaction and loyalty.\n",
       "3. **Enhanced Creativity**: LLMs can assist writers and artists in generating new ideas, sparking creativity, and exploring uncharted territories.\n",
       "\n",
       "**Challenges and Concerns**\n",
       "\n",
       "While LLMs hold immense promise, they also raise several concerns:\n",
       "\n",
       "1. **Job Displacement**: Automation could lead to job losses for human customer service representatives, writers, and other professionals.\n",
       "2. **Misinformation**: The ability of LLMs to generate text can spread misinformation and propaganda, highlighting the need for fact-checking and content moderation.\n",
       "3. **Bias and Fairness**: These models can perpetuate biases present in training data, emphasizing the importance of diverse and inclusive datasets.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Large language models have revolutionized human-computer interaction, offering unparalleled capabilities in natural language understanding and generation. While there are valid concerns surrounding their deployment, the benefits of LLMs far outweigh the risks. As we continue to harness the power of AI, it is essential that we prioritize responsible development, ensuring these models serve humanity while respecting its values.\n",
       "\n",
       "**What's Next?**\n",
       "\n",
       "The future of large language models holds tremendous potential for innovation and growth:\n",
       "\n",
       "1. **Advancements in Multimodal Models**: LLMs will increasingly incorporate multimodal inputs, such as images, audio, and video, to create more comprehensive AI experiences.\n",
       "2. **Improved Explainability and Transparency**: Researchers will focus on developing techniques to explain the reasoning behind LLM decisions, ensuring accountability and trustworthiness.\n",
       "\n",
       "Stay tuned for future updates on the world of large language models!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "context = \"Large Language Models\"\n",
    "\n",
    "chain = prompt | model_llama | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bb86b-ddaa-49d2-acf6-d25345f727d7",
   "metadata": {},
   "source": [
    "The model generates a well formatted and structured response. The output looks better that the one generated by Mistral:7b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708478c-1488-4370-bc01-5ffdb7b807f8",
   "metadata": {},
   "source": [
    "### Phi3.5\n",
    "\n",
    "Let us now test a lightweight language model. It has only 3.8 billion parameters. But it is known to overtake similar or even larger sized models. It is an open-source model developed by microsoft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a11f67a-8928-4649-896c-5d77c0eafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\") #loading phi3.5:latest form ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f3dc37-96b5-4f58-9e14-df16e6dc2e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Title: Unraveling Potential and Pitfalls of Large Language Models in Today'supublication-sized format, this comprehensive exploration delves into both opportunities and challenges presented by cutting-edge language models. Here’s why these AI behemoths are reshaping communication across industries:\n",
       "\n",
       "**Introduction - The Ascendance of Language Models (LMs) in the Tech Landscape**\n",
       "The emergence of Large Language Models has been nothing short of revolutionary, heralding a new era for natural language processing. These models boast an extensive vocabulary and can generate or comprehend text with uncanny proficiency—a leap from the previous generation's capabilities that seemed primitive by comparison.\n",
       "\n",
       "**The Power Behind LM Success: Data, Computation & Innovation** \n",
       "At their core lie massive datasets upon which they have been trained meticulously to understand and reproduce human language patterns. The sheer computational power required is unprecedented; it takes thousands of high-end processors working simultaneously for hours on end—or even days or weeks, depending on the model’s complexity (GPT models).\n",
       "\n",
       "**Beneficial Impact Across Industries: From Customer Service to Content Creation and Beyond** \n",
       "In customer service applications like chatbots, these LMs swiftly understand queries with contextual clues that often elude even seasoned human operators. They can generate engaging content for marketing campaigns or write blog articles imbued with creativity—a boon to writers who may find a new collaborative partner in AI algorithms like the one writing this piece today!\n",
       "\n",
       "**Education and Accessibility: Personalized Learning Experiences With LM Assistance** \n",
       "In educational settings, these models can provide tailored learning experiences by answering questions or offering explanations based on individual student needs. They break down complex subjects into comprehensible chunks—the epitome of virtual tutoring at one's fingertips without the hefty price tag attached to human experts.\n",
       "\n",
       "**Challenges and Ethical Considerations: A Delicate Balancing Act Amid Advancements in LM Technology** \n",
       "However, as with any powerful tool that can shape minds and opinions—language being a cornerstone of culture — there are ethical dilemmas to consider too. For instance; the potential for amplifying biases present within training data sets or issues surrounding consent when using generated content without proper citation mechanisms in place could raise questions about intellectual property rights as AI becomes more prevalent creators rather than tools alone used by humans today (Mollick, 2019).\n",
       "\n",
       "Moreover privacy concerns lurk beneath these artificial intelligence advances too. Who owns the information an algorithm processes during its training phase or when generating content based on user inputs? There have been cases where companies were accused of exploiting personal data without explicit consent—an issue that calls for strict regulatory measures to protect individual rights (Cathala, 2019).\n",
       "\n",
       "**Mitigating Bias: Unveiling the Hidden Shadows in LM Algorithms' Reflection Pools* **\n",
       "Another challenge lies within inherent biases—a reflection of past societal norms present even nowadallround us. Language models, albeit advanced ones like GPT-3 or their successors trained on diverse data sets strive to avoid such pitfalls but often find themselves embodying stereotypes and prejudices ingrained within historical texts they've learned from (Bender et al., 2019).\n",
       "\n",
       "Researchers are working tirelessly towards mitigating these biases by diversifying training datasets or developing novel algorithms capable of identifying problematic patterns—steps necessary to ensure fair representation across different demographics when deploying LMs in various sectors.\n",
       "\n",
       "**The Future Landscape: Predictive Capabilities and Transformative Potential Of Language Models* **\n",
       "Looking ahead, Large Language Model technology holds immense potential for transformational applications beyond communication—spanning healthcare diagnosis systems informed by patient history records or predictive maintenance in manufacturing through real-time analysis of operational data (Nguyen et al., 2019).\n",
       "\n",
       "In conclusion as we stand at this crossroads between human ingenuity and artificial intelligence prowess; it's imperative for stakeholders - researchers, businesses & policymakers—to collaborate in nurturing responsible deployment strategies. By addressing ethical concerns head-on while leveraging these models’ predictive capabilities we can shape a future where technology enhances humanity without compromising individual rights or perpetuating biases rooted deep within our societal fabric (Susskind & Thomas, 2019).\n",
       "**Reference:**\n",
       "Bender, S. E., et al. (2018). When Machine Learning Becomes Biased and How to Mitigate It. In Proceedings of the AAAI/ACM Conference on Artificial Intelligence, vol 32(submission date not specified) available at https://arxiv.org/abs/1610.08029\n",
       "Cathala, M., & Gandon, V. (Eds.). (2019). Ethical and Societal Aspect of Artificial Intelligence: A European Approach to Responsible Innovation in Industrial Enterprises and Public Administration Proceedings 7th International Conference on Social Impact Assessment - ICSIA.\n",
       "Nguyen, D., et al (2019). Machine Learning for Predictive Maintenance: Case Studies from Manufacturing Sector at the IEEE/ACM international conference on e-Technics and Technologies in Industry – ICT’Industria 36.\n",
       "Susskind, J., & Thomas, H.-P. (2019). Artificial intelligence and public values: Why smart machines can't make value judgments but humans must learn how to do so themselves Journal of Applied Philosophy doi:/doi/10.1111/japp.12458\n",
       "Mollick, Jesse (2019). The economist explains: Will artificial intelligence end up destroying jobs? https://www.theeconomist.com accessed on 3rd February, 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a blog post on the given context {context}\"\n",
    ")\n",
    "\n",
    "context = \"Large Language Models\"\n",
    "\n",
    "chain = prompt | model_phi | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\" : context})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d92bd4-7a9c-4df8-a78c-337a0c6607f1",
   "metadata": {},
   "source": [
    "We immediately notice the difference in response generated by phi3.5 when compared to mistral and llama. This is primarily beacuse phi3.5 is a smaller model that the other two. The model generates complicated responses that doesn't look very natural. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1a42f-d21a-46b3-8a8b-70820d5a17dd",
   "metadata": {},
   "source": [
    "## Code Generation \n",
    "\n",
    "Let us test the code generation capabilities of all these models. We give a prompt to each of these models to generate a python function, and then analyze the response of each model. Feel free to edit the prompt and make the models generate other responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f4ba9-4a9e-469f-91ec-81eb1bc5d1b6",
   "metadata": {},
   "source": [
    "### Mistral\n",
    "\n",
    "Mistral:7b is really good at coding tasks. I even comes near to CodeLlama 7b at code generation tasks while being equally good at English language. Let us put Mistral's coding abilities to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0eecba6-b137-4848-9534-0fc4d68a8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffffc2b1-ee7a-44e1-8141-555e92c8aa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Here is a Python function that uses recursion to compute the factorial of a given number:\n",
       "\n",
       "```python\n",
       "def factorial(n):\n",
       "    if n == 0 or n == 1:\n",
       "        return 1\n",
       "    else:\n",
       "        return n * factorial(n - 1)\n",
       "```\n",
       "\n",
       "You can use this function by calling `factorial(number_to_find_factorial)`. For example, `print(factorial(5))` will output `120`, which is the factorial of 5."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_mistral.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25513aaa-fa1d-4c4f-bc82-69c8a6009e24",
   "metadata": {},
   "source": [
    "It generated a pretty good response. But we can notice that it did not provide a function description, return type and the argument definitions. Apart from that, the code is straight forward and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b23257-ace3-4ad1-8b07-b8613822b872",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b \n",
    "\n",
    "LLaMA3.1:8b is a really good LLM for coding tasks. It outperforms most of the models of its size and even comes closer to some bigger models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "827e27a1-892b-4e4f-b4f5-c23a0eded2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "390ae78f-2814-47d7-ab02-e5bc40b9ba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a Python function that recursively computes the factorial of a number:\n",
       "\n",
       "```python\n",
       "def factorial(n):\n",
       "    \"\"\"\n",
       "    Recursively computes the factorial of a number.\n",
       "\n",
       "    Args:\n",
       "        n (int): The input number.\n",
       "\n",
       "    Returns:\n",
       "        int: The factorial of n.\n",
       "    \"\"\"\n",
       "    if n == 0 or n == 1:\n",
       "        return 1\n",
       "    else:\n",
       "        return n * factorial(n-1)\n",
       "```\n",
       "\n",
       "Example usage:\n",
       "\n",
       "```python\n",
       "print(factorial(5))  # Output: 120\n",
       "```\n",
       "\n",
       "This function takes an integer `n` as input, and returns its factorial. If `n` is 0 or 1, it returns 1 (since the factorial of 0 and 1 are both 1). Otherwise, it calls itself with `n-1` as argument and multiplies the result by `n`, effectively calculating the factorial.\n",
       "\n",
       "Please note that this recursive approach may lead to a stack overflow for large values of n due to the limited size of Python's call stack. If you need to compute factorials of very large numbers, an iterative solution would be more suitable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_llama.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a260262-9672-4a3f-a571-191c707b959e",
   "metadata": {},
   "source": [
    "The response generated by LLaMa is really good. It provided all the function description and argument definitions. It also generated the shortcomings of computing factorials using the recursive approach. The generated response is self explanatory, anybody reading it can understand what the code is about. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ec069-98cc-4066-8eb8-56b36869189e",
   "metadata": {},
   "source": [
    "### Phi3.5\n",
    "\n",
    "Let us test the coding abilities of a really light-weight LLM and see how it performs against larger LLMs like Mistral & LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dafb752-9946-4fa7-be31-996094696af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff611fca-e8ee-478f-9530-016a7117eaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a Python function using recursion to calculate the factorial of a non-negative integer:\n",
       "\n",
       "```python\n",
       "def factorial(n):\n",
       "    # Base case: if n is zero, its factorial is 1\n",
       "    if n == 0:\n",
       "        return 1\n",
       "    \n",
       "    # Recursive case: multiply current number with previous result (factorial of one less)\n",
       "    else:\n",
       "        return n * factorial(n-1)\n",
       "```\n",
       "To use this function, simply call it with a non-negative integer as an argument like so: `result = factorial(5)`. \n",
       "\n",
       "Remember that the recursive solution has limitations; for large values of 'n', you might encounter problems due to stack overflow. A loop based iterative method is usually more efficient and reliable in such cases, although it doesn't use recursion explicitly as requested here."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful chat assistant that generates python code for a given user query\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Write a python function that recursively compute factorial of a number\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model_phi.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da03660-d0e3-49ab-8d76-ae6e87a86ee1",
   "metadata": {},
   "source": [
    "The code and explanations that follow looks good. However, including the base case of `n == 1` is missing here. Although this doesn't change the output, it might result in an additional recursion call, unnecessarily increasing recursion depth. So the response by Mistral or LLaMA is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc33ab-11a3-4330-95ac-472d435f1120",
   "metadata": {},
   "source": [
    "## Text Summarization \n",
    "\n",
    "Large Language Models (LLMs) are highly effective for text summarization as they can grasp context and extract key information across lengthy texts. They leverage extensive training on diverse data to generate concise summaries while retaining the original meaning and essential details. LLMs handle various summarization styles, from extractive (directly pulling important sentences) to abstractive (generating novel sentences). This adaptability makes them valuable for applications across industries, from media and research to customer support and legal fields, improving efficiency in processing vast amounts of information. \n",
    "\n",
    "We provide a paragraph on HPE Proliant servers to each of these LLMS and ask them to summarize it in 2 short sentences. We can then analyze each outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e22564-8c1e-489f-aa5e-afd8d3a93498",
   "metadata": {},
   "source": [
    "### Mistral:7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9523f04-f91f-4064-827b-afdfc506644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mistral = ChatOllama(model=\"mistral:7b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dc1a7da-1d69-4646-b7e6-33131b0189e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. HPE ProLiant servers are the world's most secure industry-standard servers, available in various configurations to cater to businesses of all sizes. They offer software-defined compute solutions with features like HPE OneView, HPE InfoSight, and HPE OneSphere for enhanced application performance, deployment, and server operations.\n",
       "\n",
       "2. These servers support the leading operating systems and applications, providing excellent scalability for global enterprises while also catering to the budget needs of growing businesses. Visit hpe.com/info/proliant-dl-servers, hpe.com/info/towerservers, or hpe.com/info/bladesystem for more details."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_mistral | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8c145-05fa-40c8-8600-11fe2fb05196",
   "metadata": {},
   "source": [
    "Mistral captured all the important details in the original paragraph. But, it provided two large sentences. It wasn't able to provide a short summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072dc27-b715-41a9-b61e-1f3fbe74f0f2",
   "metadata": {},
   "source": [
    "### LLaMA3.1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afd2323f-dddc-4860-88ed-35ff7a630e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2521ecec-4f83-4c31-aa43-5c0a8d33420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a 2-sentence summary:\n",
       "\n",
       "HPE ProLiant servers offer secure and software-defined compute solutions that accelerate application performance and improve server operations. With a wide range of options, including multicore and multiprocessor servers, HPE ProLiant supports the needs of businesses and enterprises of all sizes and scales."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_llama | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a194c96-3e09-474b-845d-3d8f58c033e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phi = ChatOllama(model=\"phi3.5:latest\", base_url=\"http://10.79.253.112:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f533dc3c-644f-4ab0-a26c-1f7606b0d546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "HPE ProLiant servers offer the highest security standards for industry standard computing needs and provide software-defined compute to enhance application performance and server operations through HPE OneView, InfoSight, and OneSphere technologies; they cater to a broad range of requirements from cost-conscious businesses scaling upwards. They support leading operating systems across various data center sizes with options for multicore or multiprocessor servers suitable for both entry-level enterprises and global powerhouses demanding top performance, available at hpe.com/proliant.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, summarized version of the provided paragraph in 2 sentences {paragraph}\"\n",
    ")\n",
    "\n",
    "paragraph = \"\"\"HPE ProLiant servers—The world’s most secure industry standard servers,1\n",
    "                HPE ProLiant Gen10 and Gen10 Plus servers coupled with HPE OneView, HPE InfoSight, \n",
    "                and HPE OneSphere deliver software-defined compute to accelerate application performance, \n",
    "                infrastructure and application deployment, and improve server operations. \n",
    "                Our wide selection of multicore, multiprocessor servers, and server blades meet needs \n",
    "                ranging from those of cost-sensitive growing businesses to the performance and scalability \n",
    "                demands of global enterprises. HPE ProLiant servers support the industry’s leading operating \n",
    "                systems and applications for data centers of all sizes. hpe.com/info/ proliant-dl-servers, \n",
    "                hpe.com/info/towerservers, hpe.com/info/bladesystem\"\"\"\n",
    "\n",
    "\n",
    "chain = prompt | model_phi | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"paragraph\" : paragraph})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fec9c2-514e-4234-b18d-829f8edb345b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0445f-e8f2-4fc2-9612-94b43b673719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
