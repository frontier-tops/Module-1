{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81211ab-94cc-44b8-94c0-f247a0dd04b0",
   "metadata": {},
   "source": [
    "# Function Calling Use case with Google Gemini model\n",
    "\n",
    "## Lab Description:\n",
    "\n",
    "This lab demonstrates how to integrate function calling capabilities with the Google Gemini model. It begins by setting up the Google API key and defining a simple arithmetic function. The function is then passed as a tool during LLM initialization, enabling the model to invoke it dynamically. The lab concludes by exploring parallel function calling features of the Gemini model.\n",
    "\n",
    "## Lab Objectives:\n",
    "\n",
    "After completing the lab, the participants will be able to:\n",
    "\n",
    "- Set up and authenticate the Google Gemini API for function calling.\n",
    "- Define and integrate custom functions as tools within the Gemini model.\n",
    "- Utilize function calling during LLM execution to enhance model capabilities.\n",
    "- Explore and implement parallel function calling to handle multiple tasks efficiently.\n",
    "\n",
    "## What is function calling ?\n",
    "\n",
    "\n",
    "Function calling gfeature in the Gemini API allows the model to generate structured outputs that specify function names and arguments. The model itself doesn't invoke the functions directly but provides the necessary information to call external APIs. After the API is called using the suggested function and arguments, the output can be used to further query the model, enabling it to give more comprehensive responses.\n",
    "\n",
    "## How function calling works ? \n",
    "\n",
    "A structured query data describing the function is passed to a model prompt. It includes all the details like the function name, parameters, description of the function and parameters and so on. The model then analyzes this description to effectively provide responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d753a7-e1c3-4b80-bf20-86ff3342374a",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cff5e0-792d-416b-91d3-e93a818a3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyter/gemini_function_calling/gemini/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faed9db-f545-4627-af44-4e0b3756e8d5",
   "metadata": {},
   "source": [
    "\n",
    "## Generate a Google Studio API Key\n",
    "\n",
    "We will create an API key that provides secure access to the Gemini API, enabling the Jupyter Notebook to communicate with the servers.\n",
    "\n",
    "### Set Up an AI Studio Account\n",
    "\n",
    "1. Visit **AI Studio** and click on **\"Sign in to AI Studio\"**.  \n",
    "   \n",
    "      <img src=\"one.png\" alt=\"Figure 1: Sign in page of AI Studio\" width=\"600\" height=\"400\">\n",
    "\n",
    "### Obtain Your API Key\n",
    "\n",
    "2. After logging into AI Studio, scroll down and select **\"Get your API Key.\"**  \n",
    "   \n",
    "      <img src=\"two.png\" alt=\"Figure 1: Sign in page of AI Studio\" width=\"600\" height=\"400\">\n",
    "\n",
    "### Create the API Key\n",
    "\n",
    "3. Click on **\"Get API Key\"**, then select **\"Create API Key\"**, and then choose **\"Create API key in new project\"**.  \n",
    "   \n",
    "   <img src=\"three.png\" alt=\"Figure 1: Sign in page of AI Studio\" width=\"600\" height=\"400\">\n",
    "\n",
    "   <img src=\"four.png\" alt=\"Figure 1: Sign in page of AI Studio\" width=\"600\" height=\"400\">\n",
    "\n",
    "### Copy the API Key\n",
    "\n",
    "4. After generating the key, **copy** it for use in your Jupyter Notebook.  \n",
    "\n",
    "   <img src=\"five.png\" alt=\"Figure 1: Sign in page of AI Studio\" width=\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a20a85-31aa-4a6b-9663-184fc07a717b",
   "metadata": {},
   "source": [
    "## Setting the GOOGLE API KEY\n",
    "\n",
    "You can create tour API KEY via https://aistudio.google.com. Once your key is generated, it is required to set it as environment variable. It needs to be activated by passing it into genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3deefdd4-4447-4b3a-9254-57377a12d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = input(\"Please enter your Google API Key: \")\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f807f-e54a-4472-a4b4-e49e29a0976e",
   "metadata": {},
   "source": [
    "## Define a function \n",
    "\n",
    "Let us first define a really simple function to understad and test function calling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da903b92-ab38-4393-afbd-bbc7ec7d7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:float, b:float):\n",
    "    \"\"\"returns a + b.\"\"\"\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e003d07-258a-45c0-89f7-6b492bf5e02a",
   "metadata": {},
   "source": [
    "This is a simple function that adds two numbers. Let us see if we could make the model use this function to answer our query. To use the function calling feature, the function should be declared at the model initialization itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc790518-48c1-43f5-a889-be9c44be0056",
   "metadata": {},
   "source": [
    "## Declare function during model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96dfc0e-fa30-4dfa-9e78-68900bf43d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash', tools=[add]) #The tools param is used to declare functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d29898-3ba1-434f-a444-1a6bfb7c3b8b",
   "metadata": {},
   "source": [
    "We can now use the model to use the defined function to answer our query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43911695-7f1e-4fbd-bdd7-f53e40cdea55",
   "metadata": {},
   "source": [
    "## Generate function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4391170-0ae1-4eb5-8a31-7db4734a56ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'57 added to 22 is 79.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat(enable_automatic_function_calling=True) #enable_automatic_function_calling should be set to true to have the SDK automatically use the function \n",
    "response = chat.send_message('What is 57 added to 22 ?')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b6fc9-8674-449a-bf09-5a4eff81fb28",
   "metadata": {},
   "source": [
    "The model automatically uses the function to find answer to our query. Let us now see what the flow is like to confirm if the model is actually using the function and not just generating answer on it's own. \n",
    "\n",
    "chat.history contains the sequence of these events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38caad49-38c1-4915-a4f2-8641507a7ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user -> {'text': 'What is 57 added to 22 ?'}\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "model -> {'function_call': {'name': 'add', 'args': {'a': 57.0, 'b': 22.0}}}\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "user -> {'function_response': {'name': 'add', 'response': {'result': 79.0}}}\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "model -> {'text': '57 added to 22 is 79.'}\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for content in chat.history:\n",
    "    part = content.parts[0]\n",
    "    print(content.role, \"->\", type(part).to_dict(part))\n",
    "    print('x'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba134c-8fbe-453f-b060-8c7a4672151b",
   "metadata": {},
   "source": [
    "We can see that after processing user query, the model calls the function and identifies the parameters. It then executes the function locally to generate a response. \n",
    "\n",
    "This proves that function calling is actually working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17419e98-1999-4b85-a6ed-c3c07c072802",
   "metadata": {},
   "source": [
    "## Parallel Function Calling\n",
    "\n",
    "In addition to normal function calling, you can also call multiple functions in a single turn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8886f8a-39a7-4958-898a-9a02d24ce415",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ad9a6f-bd07-458c-9e07-ea550ed897f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_disco_ball(power: bool) -> bool:\n",
    "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
    "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      energetic: Whether the music is energetic or not.\n",
    "      loud: Whether the music is loud or not.\n",
    "      bpm: The beats per minute of the music.\n",
    "\n",
    "    Returns: The name of the song being played.\n",
    "    \"\"\"\n",
    "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
    "    return \"Never gonna give you up.\"\n",
    "\n",
    "\n",
    "def dim_lights(brightness: float) -> bool:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "    \"\"\"\n",
    "    print(f\"Lights are now set to {brightness:.0%}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba39c97-f25b-4ce7-acd5-b6f1077b6ffa",
   "metadata": {},
   "source": [
    "## Declare the functions during model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ac35b-ef45-4a80-a992-7eb61198db36",
   "metadata": {},
   "source": [
    "Define tools: The house_fns list contains a set of functions that might be used to control a party atmosphere—specifically, power_disco_ball, start_music, and dim_lights.\n",
    "\n",
    "Initialize model: The GenerativeModel is initialized with the model name \"gemini-1.5-flash\", and the previously defined tools (house_fns) are passed in as available functions for this model to use.\n",
    "\n",
    "Start chat session: A chat session is started with the start_chat() method of the model, creating a session where the model can process and respond to messages.\n",
    "\n",
    "Send message: The message \"Turn this place into a party!\" is sent to the model via the send_message method. The model generates a response based on the message and decides which of the available tools should be called.\n",
    "\n",
    "Function extraction and printing: The response from the model is examined part by part. If any function calls are present (i.e., the model decides to use one of the tools), the function name and its arguments are extracted and printed in a readable format, showing which functions the model called and with what arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8ecbe0-079c-4837-881f-01c72d0a8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_disco_ball(power=True)\n",
      "start_music(loud=True, energetic=True, bpm=120.0)\n",
      "dim_lights(brightness=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Set the model up with tools.\n",
    "house_fns = [power_disco_ball, start_music, dim_lights]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
    "\n",
    "# Call the API.\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call.\n",
    "for part in response.parts:\n",
    "    if fn := part.function_call:\n",
    "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "        print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bda40-ebea-4653-b245-8acc41c8deaa",
   "metadata": {},
   "source": [
    "Each of this response reflects a single function call. To send the resoonses back, it should be simulated in the same order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ae6a8-3a86-475f-bac1-d75ceb59473d",
   "metadata": {},
   "source": [
    "## Simulating and building the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144e36eb-28aa-407e-a5f3-91301418be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've turned on the disco ball, started playing some energetic music at 120 bpm, and dimmed the lights. Let's get this party started! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate the responses from the specified tools.\n",
    "responses = {\n",
    "    \"power_disco_ball\": True,\n",
    "    \"start_music\": \"Never gonna give you up.\",\n",
    "    \"dim_lights\": True,\n",
    "}\n",
    "\n",
    "# Build the response parts.\n",
    "response_parts = [\n",
    "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
    "    for fn, val in responses.items()\n",
    "]\n",
    "\n",
    "response = chat.send_message(response_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543a4aa-d505-43a7-abf0-301c27bee5a6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26026953-7250-4211-b10d-b26d580a878c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
