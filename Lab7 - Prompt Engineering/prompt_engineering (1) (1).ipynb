{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b1a2e4-aab2-48a5-8a63-a70b365d52cc",
   "metadata": {},
   "source": [
    "# Prompt Engineering and Prompting Techniques\n",
    "\n",
    "Prompt engineering involves designing and refining input prompts to effectively communicate with language models, optimizing their performance on specific tasks. By carefully crafting these prompts, users can guide models to produce more accurate and relevant outputs, enhancing their utility in applications like chatbots and content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639d85b-cf13-4445-a8f7-74fed2f212a2",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab explores effective prompting techniques for Large Language Models. It starts with basic text completion, text summarization and sentiment analysis. Participants will then experiment with code genera|tion by prompting the LLM to generate neat and correct code both in Python and SQL. Finally, the lab covers advanced prompting techniques, including few-shot and chain-of-thought prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23de70-5c95-4627-ab50-d0cdef923df1",
   "metadata": {},
   "source": [
    "## Lab Objectives:\n",
    "\n",
    "After completion of the lab, the participants will be able to:\n",
    "\n",
    "- Effectively construct prompts for basic text completion, sentiment analysis, code generation, and text summarization.\n",
    "  \n",
    "- Prompt the LLM to write neat and correct Code.\n",
    "  \n",
    "- Utilize advanced prompting techniques, including few-shot and chain-of-thought prompting, to improve model accuracy and reasoning.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9344236-d474-492e-ade0-59ee57c3d8cb",
   "metadata": {},
   "source": [
    "## Importing the Libraries\n",
    "\n",
    "This lab requires the following libraries:\n",
    "\n",
    "- `langchain_community` : This library contains the `Ollama` method which we leverage to connect to the LLM hosted on Ollama.\n",
    "- `langchain_core` : This library contains the methods like `StrOutputParser` , `HumanMessage` and `SystemMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482c2a90-1893-416f-b388-0e7fdb67a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama                     #Importing the Ollama library for langchain\n",
    "from langchain_core.output_parsers import StrOutputParser       #Importing the stringoutput parser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage #Langchain messages class provides a way to communicate with the LLM, The HumanMessage corresponds to the human message string (like the questions to the llm) and the SystemMessages corresponds to the instructions to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf352bb-464e-4e1e-9303-2b6cc6bca27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model = \"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\", temperature=0.0)   # Loading the ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea15aa-4516-430c-849f-bfc97a8a3188",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Basic chat completion involves providing a prompt to a Large Language Model (LLM) for making it predict the next word in a sentence. The model completes the given sentence in a conversational manner by analyzing context and language patterns. This forms the foundation for more advanced prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd360341-72cf-48fc-9186-4ebdc7eca01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Complete the given sentence in one word\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"The sky is\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2899ee-2766-43aa-b318-1d6fd32c4bd9",
   "metadata": {},
   "source": [
    "This can also be done using prompt templates. Langchain provides a prompttemplate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dfd6250-9504-4adb-9f51-ff3193063e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec367b1c-194a-458e-aa79-7f055f1e8f24",
   "metadata": {},
   "source": [
    "## Components of a Basic Prompt Template\n",
    "\n",
    "A basic prompt template usually consists of and instruction message followed by a Placeholder. The Placeholder can be anything varying from text that needs to be summarized, retrieved documents and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325299d-584e-4697-ae1d-f4ab009daea9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"prompt_template.png\" alt=\"flow\" width=\"700\" height=\"450\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ac229d-66da-4cdf-a6e8-b50cd273a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(               #The .from_template() method loads a prompt template from a template(in this case, the string we provided)\n",
    "    \"\"\"Complete the following sentence in one word      \n",
    "\n",
    "       sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "sentence = \"The sky is\"                              #The sentence to format the prompt\n",
    "\n",
    "chain = prompt | model | StrOutputParser()           #Building the chain\n",
    "\n",
    "response = chain.invoke({\"sentence\":sentence})       #Invoking the chain\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ae7ea-d6fa-4f90-bf5a-eb5153876afe",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "LLMs are widely used for text summarization tasks. This section introduces prompts that could efficiently guide the LLMs to summarize a given chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169d22df-16fe-4c0f-82a2-0c3ec1c63037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a 2-sentence summary:\\n\\nGenerative AI refers to artificial intelligence techniques that can create new content such as text, images, music, and more by learning patterns from existing data. These models have vast applications in various industries, but also raise important questions about originality, ethics, and potential misuse.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Summarize the given context in 2 sentences at most\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content= \"Generative AI refers to a class of artificial intelligence techniques that can create new content, such as text, images, music, and more, by learning patterns from existing data. These models, like GPT and DALL-E, utilize deep learning algorithms to generate outputs that can mimic human creativity and style. The applications of generative AI are vast, ranging from content creation and virtual art to drug discovery and game design. As this technology continues to evolve, it raises important questions about originality, ethics, and the potential for misuse. Overall, generative AI is transforming various industries by enabling new forms of creativity and automation.\"\n",
    "                    \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf384da-19c2-43fa-a752-dd55196d861b",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "LLMs are quite good at Question Answering. This section demonstrates prompts that could efficiently make LLMs generate meaningful, accurate and concise answers to user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af9789d-9d3a-4b87-805d-eb6260948f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By carefully crafting the wording, structure, and context of prompts to effectively communicate with language models.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template to guide the model's responses based on a provided context\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the context below. Be short and concise. Reply \"Unsure\" if you are not sure about the answer. Donot make up answers.\n",
    "\n",
    "       Context: Prompt engineering is the process of designing input prompts to effectively communicate with language models, optimizing their outputs for specific tasks. By carefully crafting the wording, structure, and context of prompts, users can enhance the accuracy and relevance of the model's responses. This technique is crucial in applications like chatbots and content generation, where the quality of the input directly influences the quality of the generated content.\n",
    "\n",
    "       Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the question to be asked based on the context\n",
    "question = \"How can prompt engineers enhance the quality of the model's response?\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the question and store the response\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6f054-0930-48b6-8815-785f6441a3d7",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "This section demonstrates prompting to make the model classify a sentence as positive, neutral or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b4ca8c-3099-4042-afc0-016982b520fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template to classify a given sentence as positive, neutral, or negative\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the following sentence into positive, neutral or negative. Do not explain how you arrived at the answer.\n",
    "\n",
    "       Sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the sentence to be classified\n",
    "sentence = \"The burgers were really tasty.\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the sentence and store the response\n",
    "response = chain.invoke({\"sentence\": sentence})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0a482-b3f1-42b9-9d36-5f5b13e294c7",
   "metadata": {},
   "source": [
    "#### Trying with a negative sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667277a2-10a7-4a9e-bc27-9ad9ffb5abb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template to classify a given sentence as positive, neutral, or negative\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the following sentence into positive, neutral or negative. Do not explain how you arrived at the answer.\n",
    "\n",
    "       Sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the sentence to be classified\n",
    "sentence = \"The service was not upto the mark\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the sentence and store the response\n",
    "response = chain.invoke({\"sentence\": sentence})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedf9d5-6d2a-445d-a523-2445a96f28d8",
   "metadata": {},
   "source": [
    "## CODE GENERATION\n",
    "\n",
    "### SQL QUERY\n",
    "\n",
    "Here, we provide the LLM with a table and an instruction to generate a SQL Query based on that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3285dea-f93d-452f-8f66-2dbf384277d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT E.FirstName, E.LastName, D.DepartmentName, E.Salary\n",
       "FROM Employees E\n",
       "JOIN Departments D ON E.DepartmentId = D.DepartmentId\n",
       "WHERE D.DepartmentName = 'Sales';\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list of messages for the model to process\n",
    "messages = [\n",
    "    # System message providing instructions to the LLM\n",
    "    SystemMessage(\n",
    "        content=\"Create a SQL query to find the names and salaries of all employees who work in the 'Sales' department, with respect to the table details given. Generate only the query, no explanation is needed.\"\n",
    "    ),\n",
    "    # Human message providing the context and table details\n",
    "    HumanMessage(\n",
    "        content= \"\"\"\n",
    "                 Table Employees, columns = [EmployeeId, FirstName, LastName, DepartmentId, Salary]\n",
    "                 Table Departments, columns = [DepartmentId, DepartmentName]\n",
    "                 \"\"\"                \n",
    "    )\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the generated SQL query as formatted Markdown\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdad829-087f-4c81-8034-8b7cc6399c1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### PYTHON\n",
    "\n",
    "Here we provide a simple code generation prompt to make the LLM generate a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "692af1b2-c1e2-48c8-a103-643e202d1d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def reverse_string(input_str):\n",
       "    \"\"\"\n",
       "    This function takes an input string and returns its reversed version.\n",
       "    \n",
       "    Parameters:\n",
       "    input_str (str): The string to be reversed.\n",
       "    \n",
       "    Returns:\n",
       "    str: The reversed string.\n",
       "    \"\"\"\n",
       "    \n",
       "    # Initialize an empty string to store the reversed string\n",
       "    reversed_str = \"\"\n",
       "    \n",
       "    # Iterate over each character in the input string\n",
       "    for char in input_str:\n",
       "        # Add each character at the beginning of the reversed string (equivalent to reversing)\n",
       "        reversed_str = char + reversed_str\n",
       "    \n",
       "    # Return the reversed string\n",
       "    return reversed_str\n",
       "\n",
       "# Test the function with a sample string\n",
       "input_string = \"Hello, World!\"\n",
       "print(\"Original String:\", input_string)\n",
       "print(\"Reversed String:\", reverse_string(input_string))\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a prompt template to generate a Python method based on the provided question\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Generate python method for the asked question. Provide the code with comments. Do not explain the code\n",
    "    \n",
    "       Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the question to be asked\n",
    "question = \"Reverse a string\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the question and store the response\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# Display the generated Python code as formatted Markdown\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff7881-b1d1-4904-91d8-fc114f6b4411",
   "metadata": {},
   "source": [
    "## ADVANCED PROMPTING TECHNIQUES\n",
    "\n",
    "### Few-Shot Prompts\n",
    "\n",
    "Few-shot prompts are a technique in natural language processing that provides a model with a small number of examples (or \"shots\") to guide its responses for a specific task. This approach allows the model to learn from the provided examples and generate outputs that align with the desired format or content, improving its performance on tasks with limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a859c54d-8416-431d-9d57-9cab292934d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It looks like you have a series of questions about whether the sum of the multiples of 3 in each group is greater than 20. I'll answer them for you.\n",
       "\n",
       "Q: The sum of the multiples of 3 in this group is greater than 20: 3, 7, 9, 12, 5, 14.\n",
       "Answer: True\n",
       "\n",
       "The multiples of 3 in this group are 3, 9, and 12. Their sum is 24, which is indeed greater than 20.\n",
       "\n",
       "Q: The sum of the multiples of 3 in this group is greater than 20: 6, 4, 9, 15, 2, 18.\n",
       "Answer: True\n",
       "\n",
       "The multiples of 3 in this group are 6, 9, and 18. Their sum is 33, which is indeed greater than 20.\n",
       "\n",
       "Q: The sum of the multiples of 3 in this group is greater than 20: 1, 2, 4, 6, 3, 10.\n",
       "Answer: False\n",
       "\n",
       "The multiples of 3 in this group are 3 and 6. Their sum is 9, which is not greater than 20.\n",
       "\n",
       "Q: The sum of the multiples of 3 in this group is greater than 20: 9, 18, 5, 7, 21, 12.\n",
       "Answer: True\n",
       "\n",
       "The multiples of 3 in this group are 9, 18, and 21. Their sum is 48, which is indeed greater than 20.\n",
       "\n",
       "Q: The sum of the multiples of 3 in this group is greater than 20: 8, 3, 14, 27, 5, 6.\n",
       "Answer: True\n",
       "\n",
       "The multiples of 3 in this group are 3, 6, and 27. Their sum is 36, which is indeed greater than 20."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a few-shot prompt containing multiple questions and answers about sums of multiples of 3\n",
    "prompt = \"\"\"Q: The sum of the multiples of 3 in this group is greater than 20: 3, 7, 9, 12, 5, 14.\n",
    "            Answer:True\n",
    "\n",
    "            Q: The sum of the multiples of 3 in this group is greater than 20: 6, 4, 9, 15, 2, 18.\n",
    "            Answer:True\n",
    "\n",
    "            Q: The sum of the multiples of 3 in this group is greater than 20: 1, 2, 4, 6, 3, 10.\n",
    "            Answer:False\n",
    "\n",
    "            Q: The sum of the multiples of 3 in this group is greater than 20: 9, 18, 5, 7, 21, 12.\n",
    "            Answer:True\n",
    "\n",
    "            Q: The sum of the multiples of 3 in this group is greater than 20: 8, 3, 14, 27, 5, 6.\n",
    "            Answer: \"\"\"  # Few-shot prompt providing context for the model\n",
    "\n",
    "# Prepare the message for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131276c0-e839-4074-8b0b-f52f3bdd16e0",
   "metadata": {},
   "source": [
    "### Chain-Of-Thought Prompting\n",
    "\n",
    "Chain of thought prompting is a technique in natural language processing that encourages a language model to break down its reasoning process step-by-step before arriving at a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e6d6e84-0140-4923-bd73-def53b382f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the statement:\n",
       "\n",
       "The sum of the multiples of 3 in this group is greater than 20: 1, 2, 4, 6, 3, 10.\n",
       "\n",
       "Multiples of 3 in the group are: 3 and 6 and 9 (wait, there is no 9... but we can add 10 to get a multiple of 3, which is 30. So, let's just say that 10 is also a multiple of 3).\n",
       "\n",
       "The sum of these multiples of 3 is: 3 + 6 + 10 = 19.\n",
       "\n",
       "Since the sum (19) is not greater than 20, the statement is False.\n",
       "\n",
       "So, the correct answer is:\n",
       "\n",
       "A: The answer is False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a prompt that provides an example of calculating the sum of multiples of 3\n",
    "prompt = \"\"\"The sum of the multiples of 3 in this group is greater than 20: 3, 7, 9, 12, 5, 14.\n",
    "A: Adding all multiples of 3 (3,9,12) gives 24. The answer is True.\n",
    "\n",
    "The sum of the multiples of 3 in this group is greater than 20: 1, 2, 4, 6, 3, 10.\n",
    "A:\"\"\"  # The prompt includes a completed example and an incomplete one for the model to finish\n",
    "\n",
    "# Prepare the message for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9de463-0d16-4ea4-8689-cf39b5507cd5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bef2a3-cbe7-4dde-8064-aa65e87b3040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
