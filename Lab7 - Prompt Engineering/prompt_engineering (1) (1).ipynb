{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b1a2e4-aab2-48a5-8a63-a70b365d52cc",
   "metadata": {},
   "source": [
    "# Prompt Engineering and Prompting Techniques\n",
    "\n",
    "Prompt engineering involves designing and refining input prompts to effectively communicate with language models, optimizing their performance on specific tasks. By carefully crafting these prompts, users can guide models to produce more accurate and relevant outputs, enhancing their utility in applications like chatbots and content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639d85b-cf13-4445-a8f7-74fed2f212a2",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab explores effective prompting techniques for Large Language Models. It starts with basic text completion, text summarization and sentiment analysis. Participants will then experiment with code genera|tion by prompting the LLM to generate neat and correct code both in Python and SQL. Finally, the lab covers advanced prompting techniques, including few-shot and chain-of-thought prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23de70-5c95-4627-ab50-d0cdef923df1",
   "metadata": {},
   "source": [
    "## Lab Objectives:\n",
    "\n",
    "After completion of the lab, the participants will be able to:\n",
    "\n",
    "- Effectively construct prompts for basic text completion, sentiment analysis, code generation, and text summarization.\n",
    "  \n",
    "- Prompt the LLM to write neat and correct Code.\n",
    "  \n",
    "- Utilize advanced prompting techniques, including few-shot and chain-of-thought prompting, to improve model accuracy and reasoning.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9344236-d474-492e-ade0-59ee57c3d8cb",
   "metadata": {},
   "source": [
    "## Importing the Libraries\n",
    "\n",
    "This lab requires the following libraries:\n",
    "\n",
    "- `langchain_community` : This library contains the `Ollama` method which we leverage to connect to the LLM hosted on Ollama.\n",
    "- `langchain_core` : This library contains the methods like `StrOutputParser` , `HumanMessage` and `SystemMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c2a90-1893-416f-b388-0e7fdb67a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama                     #Importing the Ollama library for langchain\n",
    "from langchain_core.output_parsers import StrOutputParser       #Importing the stringoutput parser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage #Langchain messages class provides a way to communicate with the LLM, The HumanMessage corresponds to the human message string (like the questions to the llm) and the SystemMessages corresponds to the instructions to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf352bb-464e-4e1e-9303-2b6cc6bca27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model = \"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\", temperature=0.0)   # Loading the ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea15aa-4516-430c-849f-bfc97a8a3188",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Basic chat completion involves providing a prompt to a Large Language Model (LLM) for making it predict the next word in a sentence. The model completes the given sentence in a conversational manner by analyzing context and language patterns. This forms the foundation for more advanced prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd360341-72cf-48fc-9186-4ebdc7eca01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Complete the given sentence in one word\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"The sky is\"                                #The human Question \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)                           #Invokes the chain with the message we designed\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2899ee-2766-43aa-b318-1d6fd32c4bd9",
   "metadata": {},
   "source": [
    "This can also be done using prompt templates. Langchain provides a prompttemplate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd6250-9504-4adb-9f51-ff3193063e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec367b1c-194a-458e-aa79-7f055f1e8f24",
   "metadata": {},
   "source": [
    "## Components of a Basic Prompt Template\n",
    "\n",
    "A basic prompt template usually consists of and instruction message followed by a Placeholder. The Placeholder can be anything varying from text that needs to be summarized, retrieved documents and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325299d-584e-4697-ae1d-f4ab009daea9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"prompt_template.png\" alt=\"flow\" width=\"700\" height=\"450\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac229d-66da-4cdf-a6e8-b50cd273a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(               #The .from_template() method loads a prompt template from a template(in this case, the string we provided)\n",
    "    \"\"\"Complete the following sentence in one word      \n",
    "\n",
    "       sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "sentence = \"The sky is\"                              #The sentence to format the prompt\n",
    "\n",
    "chain = prompt | model | StrOutputParser()           #Building the chain\n",
    "\n",
    "response = chain.invoke({\"sentence\":sentence})       #Invoking the chain\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ae7ea-d6fa-4f90-bf5a-eb5153876afe",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "LLMs are widely used for text summarization tasks. This section introduces prompts that could efficiently guide the LLMs to summarize a given chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d22df-16fe-4c0f-82a2-0c3ec1c63037",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Summarize the given context in 2 sentences at most\"   #Instruction to the LLM\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content= \"Generative AI refers to a class of artificial intelligence techniques that can create new content, such as text, images, music, and more, by learning patterns from existing data. These models, like GPT and DALL-E, utilize deep learning algorithms to generate outputs that can mimic human creativity and style. The applications of generative AI are vast, ranging from content creation and virtual art to drug discovery and game design. As this technology continues to evolve, it raises important questions about originality, ethics, and the potential for misuse. Overall, generative AI is transforming various industries by enabling new forms of creativity and automation.\"\n",
    "                    \n",
    "    )\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf384da-19c2-43fa-a752-dd55196d861b",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "LLMs are quite good at Question Answering. This section demonstrates prompts that could efficiently make LLMs generate meaningful, accurate and concise answers to user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9789d-9d3a-4b87-805d-eb6260948f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to guide the model's responses based on a provided context\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the context below. Be short and concise. Reply \"Unsure\" if you are not sure about the answer. Donot make up answers.\n",
    "\n",
    "       Context: Prompt engineering is the process of designing input prompts to effectively communicate with language models, optimizing their outputs for specific tasks. By carefully crafting the wording, structure, and context of prompts, users can enhance the accuracy and relevance of the model's responses. This technique is crucial in applications like chatbots and content generation, where the quality of the input directly influences the quality of the generated content.\n",
    "\n",
    "       Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the question to be asked based on the context\n",
    "question = \"How can prompt engineers enhance the quality of the model's response?\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the question and store the response\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6f054-0930-48b6-8815-785f6441a3d7",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "This section demonstrates prompting to make the model classify a sentence as positive, neutral or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4ca8c-3099-4042-afc0-016982b520fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to classify a given sentence as positive, neutral, or negative\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the following sentence into positive, neutral or negative. Do not explain how you arrived at the answer.\n",
    "\n",
    "       Sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the sentence to be classified\n",
    "sentence = \"The burgers were really tasty.\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the sentence and store the response\n",
    "response = chain.invoke({\"sentence\": sentence})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0a482-b3f1-42b9-9d36-5f5b13e294c7",
   "metadata": {},
   "source": [
    "#### Trying with a negative sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667277a2-10a7-4a9e-bc27-9ad9ffb5abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to classify a given sentence as positive, neutral, or negative\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the following sentence into positive, neutral or negative. Do not explain how you arrived at the answer.\n",
    "\n",
    "       Sentence: {sentence}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the sentence to be classified\n",
    "sentence = \"The service was not upto the mark\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the sentence and store the response\n",
    "response = chain.invoke({\"sentence\": sentence})\n",
    "\n",
    "# Output the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedf9d5-6d2a-445d-a523-2445a96f28d8",
   "metadata": {},
   "source": [
    "## CODE GENERATION\n",
    "\n",
    "### SQL QUERY\n",
    "\n",
    "Here, we provide the LLM with a table and an instruction to generate a SQL Query based on that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3285dea-f93d-452f-8f66-2dbf384277d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of messages for the model to process\n",
    "messages = [\n",
    "    # System message providing instructions to the LLM\n",
    "    SystemMessage(\n",
    "        content=\"Create a SQL query to find the names and salaries of all employees who work in the 'Sales' department, with respect to the table details given. Generate only the query, no explanation is needed.\"\n",
    "    ),\n",
    "    # Human message providing the context and table details\n",
    "    HumanMessage(\n",
    "        content= \"\"\"\n",
    "                 Table Employees, columns = [EmployeeId, FirstName, LastName, DepartmentId, Salary]\n",
    "                 Table Departments, columns = [DepartmentId, DepartmentName]\n",
    "                 \"\"\"                \n",
    "    )\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the generated SQL query as formatted Markdown\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdad829-087f-4c81-8034-8b7cc6399c1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### PYTHON\n",
    "\n",
    "Here we provide a simple code generation prompt to make the LLM generate a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692af1b2-c1e2-48c8-a103-643e202d1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a Python method based on the provided question\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Generate python method for the asked question. Provide the code with comments. Do not explain the code\n",
    "    \n",
    "       Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Define the question to be asked\n",
    "question = \"Reverse a string\"\n",
    "\n",
    "# Create a chain that combines the prompt with the model and parser for the response\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with the question and store the response\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# Display the generated Python code as formatted Markdown\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff7881-b1d1-4904-91d8-fc114f6b4411",
   "metadata": {},
   "source": [
    "## ADVANCED PROMPTING TECHNIQUES\n",
    "\n",
    "### Few-Shot Prompts\n",
    "\n",
    "Few-shot prompts are a technique in natural language processing that provides a model with a small number of examples (or \"shots\") to guide its responses for a specific task. This approach allows the model to learn from the provided examples and generate outputs that align with the desired format or content, improving its performance on tasks with limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859c54d-8416-431d-9d57-9cab292934d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few-shot prompt \n",
    "prompt = \"\"\"Q: This customer account should be prioritized for follow-up based on recent high-value purchases and active interest: \n",
    "            Customer Activity - [Compute Purchases: $35,000, $40,000], [Storage Purchases: $20,000], [Inquiries: 3 in the past month]\n",
    "            Answer: True\n",
    "\n",
    "            Q: This customer account should be prioritized for follow-up based on recent high-value purchases and active interest: \n",
    "            Customer Activity - [Compute Purchases: $8,000, $10,000], [Storage Purchases: $5,000], [Inquiries: 1 in the past month]\n",
    "            Answer: False\n",
    "\n",
    "            Q: This customer account should be prioritized for follow-up based on recent high-value purchases and active interest: \n",
    "            Customer Activity - [Compute Purchases: $60,000], [Storage Purchases: $30,000, $25,000], [Inquiries: 5 in the past month]\n",
    "            Answer: True\n",
    "\n",
    "            Q: This customer account should be prioritized for follow-up based on recent high-value purchases and active interest: \n",
    "            Customer Activity - [Compute Purchases: $5,000], [Storage Purchases: $4,000], [Inquiries: 0 in the past month]\n",
    "            Answer: False\n",
    "\n",
    "            Q: This customer account should be prioritized for follow-up based on recent high-value purchases and active interest: \n",
    "            Customer Activity - [Compute Purchases: $50,000, $10,000], [Storage Purchases: $12,000], [Inquiries: 4 in the past month]\n",
    "            Answer: \"\"\"  # Few-shot prompt providing context for the model\n",
    "\n",
    "# Prepare the message for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131276c0-e839-4074-8b0b-f52f3bdd16e0",
   "metadata": {},
   "source": [
    "### Chain-Of-Thought Prompting\n",
    "\n",
    "Chain of thought prompting is a technique in natural language processing that encourages a language model to break down its reasoning process step-by-step before arriving at a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d6e84-0140-4923-bd73-def53b382f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt \n",
    "prompt = \"\"\"This customer account should be prioritized for follow-up due to high-value transactions and engagement: \n",
    "Customer Activity - [Compute Purchases: $35,000, $40,000], [Storage Purchases: $20,000], [Inquiries: 3 in the past month].\n",
    "A: Adding the compute purchases ($35,000 + $40,000 = $75,000) and storage purchases ($20,000), the total is $95,000. Given 3 recent inquiries, this account shows high activity and potential. The answer is True.\n",
    "\n",
    "This customer account should be prioritized for follow-up due to high-value transactions and engagement: \n",
    "Customer Activity - [Compute Purchases: $8,000, $10,000], [Storage Purchases: $5,000], [Inquiries: 1 in the past month].\n",
    "A:\"\"\"  # The prompt includes a completed example and an incomplete one for the model to finish\n",
    "\n",
    "\n",
    "# Prepare the message for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba1208-b84b-4a42-8a27-670a62d000e9",
   "metadata": {},
   "source": [
    "## Meta Prompt Characteristics\n",
    "Structure-Oriented: The prompt focuses on the template and structure of responses rather than specific content. Each part of the response (salutation, acknowledgment, etc.) follows a standard format that applies broadly across inquiries.\n",
    "\n",
    "Syntax-Focused: Emphasis on response syntax, with placeholders (e.g., [Product Category], [specific need]) guiding the model on how to fill in details based on the customer’s inquiry.\n",
    "\n",
    "Abstract Examples: Instead of real customer inquiries, abstract labels (like [Customer Name], [Product Category]) are used, showing the form the response should take without tying it to specific details.\n",
    "\n",
    "Categorical Approach: The prompt categorizes the response elements (salutation, acknowledgment, etc.), ensuring each part logically follows from the previous and maintains a clear, organized structure.\n",
    "\n",
    "Versatility: This approach allows the model to generate relevant responses across a variety of tech sales inquiries, adapting the structure for compute, storage, or mixed inquiries as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6ac6f-470f-40c3-8cce-1e5809f92ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Meta prompt \n",
    "meta_prompt = \"\"\"This prompt outlines a structured template for responding to customer inquiries about tech solutions, focusing on syntax and response structure rather than specific examples.\n",
    "\n",
    "Template for Response:\n",
    "1. **Salutation**: Address the customer professionally.\n",
    "2. **Acknowledgment**: Recognize the customer’s specific area of inquiry (e.g., compute solutions, storage expansion).\n",
    "3. **Summary of Solution Options**: Outline the types of solutions that the company offers in the relevant category, such as compute power, storage capacity, and customization options.\n",
    "4. **Inquiry Expansion**: Suggest additional information or clarifications that may benefit the customer, such as capacity needs, scalability expectations, or budget considerations.\n",
    "5. **Conclusion and Call to Action**: End with an invitation to continue the conversation or schedule a call.\n",
    "\n",
    "Example Structure (Abstracted):\n",
    "Salutation:\n",
    "Hello [Customer Name],\n",
    "\n",
    "Acknowledgment:\n",
    "Thank you for reaching out regarding [Product Category]. \n",
    "\n",
    "Summary of Solution Options:\n",
    "We offer a variety of options tailored for [specific need, e.g., high-performance compute or scalable storage], each designed to enhance [relevant business outcomes, e.g., data processing speed, storage capacity, etc.]. Our solutions include [broad solution types].\n",
    "\n",
    "Inquiry Expansion:\n",
    "To ensure we recommend the best fit, it would be helpful to know more about your [clarifying aspect, such as compute capacity or data storage needs].\n",
    "\n",
    "Conclusion and Call to Action:\n",
    "Please feel free to reach out if you have additional questions or to schedule a meeting at your convenience.\n",
    "\n",
    "[Your Name]\n",
    "\n",
    "Application Scenario:\n",
    "- For an inquiry about high-performance compute: Structure the response to include potential compute configurations, performance details, and request for further clarification on workload requirements.\n",
    "- For a question on storage scalability: Include structured information on storage tiers, expansion capabilities, and potential customization based on data retention policies. \"\"\"  \n",
    "\n",
    "# Prepare the message for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": meta_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267cbec-4a23-43f3-b77e-2fb87ab35b0a",
   "metadata": {},
   "source": [
    "## Prompt Chain\n",
    "### Prompt 1: Extracting Relevant Product Details\n",
    "The first prompt identifies key products and specifications related to compute power and storage that might interest the customer, based on a document containing product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d8560-9ca0-461c-8d0f-40daf15dbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prompt 1 for extracting product-specific information relevant to the customer's inquiry\n",
    "prompt1 = \"\"\"You are a knowledgeable tech sales assistant. Your task is to identify product specifications related to data center compute power and storage options from the document below. Extract only relevant information on compute power, scalability, and storage capacity, and list them within <products></products> tags.\n",
    "####\n",
    "{{product_catalog}}\n",
    "####\n",
    "\n",
    "Respond with \"No relevant information found!\" if no relevant details are present.\n",
    "\"\"\"\n",
    "\n",
    "# Example document for context\n",
    "product_catalog = \"\"\"\n",
    "- High-Performance Compute Unit X1000: 128 cores, 512 GB RAM, scalable up to 1024 cores.\n",
    "- Storage Array Model S5000: 500 TB capacity, scalable to 1 PB, high-speed data transfer.\n",
    "- Compute Unit Z200: 64 cores, 256 GB RAM, designed for AI workloads.\n",
    "- Storage Expansion Model E3000: 300 TB capacity, multi-tier storage options, compatible with hybrid cloud setups.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the message for Prompt 1\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt1.replace(\"{{product_catalog}}\", product_catalog)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response1 = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed958be-790d-4e95-89e0-7e38229673b6",
   "metadata": {},
   "source": [
    "### Prompt 2: Generating the Customized Response\n",
    "The second prompt uses the extracted information from Prompt 1 to create a tailored response for the customer, combining relevant details into a coherent message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb7d77-0947-4a4f-9674-2a2a9985d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prompt 2 for generating a customer-facing response based on extracted product details\n",
    "prompt2 = \"\"\"Using the product information provided below, please generate a detailed and helpful response to the customer's inquiry. The response should summarize compute and storage options available, their scalability, and any unique features.\n",
    "####\n",
    "<products>\n",
    "{{product_details}}\n",
    "</products>\n",
    "####\n",
    "\n",
    "Make sure the tone is friendly, professional, and informative. Offer a suggestion for further conversation if the customer would like more in-depth information or assistance.\n",
    "\"\"\"\n",
    "\n",
    "# Example extracted product details from Prompt 1\n",
    "product_details = \"\"\"\n",
    "- High-Performance Compute Unit X1000: 128 cores, 512 GB RAM, scalable up to 1024 cores.\n",
    "- Storage Array Model S5000: 500 TB capacity, scalable to 1 PB, high-speed data transfer.\n",
    "- Compute Unit Z200: 64 cores, 256 GB RAM, designed for AI workloads.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the message for Prompt 2\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt2.replace(\"{{product_details}}\", product_details)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages and store the response\n",
    "response2 = model.invoke(messages)\n",
    "\n",
    "# Display the response as formatted Markdown\n",
    "display(Markdown(response2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a916237-b054-4b30-a530-237f8dc8310a",
   "metadata": {},
   "source": [
    "#### Explanation of the Prompt Chain\n",
    "Prompt 1 focuses on extracting relevant product information (compute power, storage capacity, scalability) from a larger document, simplifying the data available for the response.\n",
    "\n",
    "Prompt 2 uses the output of Prompt 1 to create a customized and coherent response for the customer, summarizing the extracted information in a friendly and professional tone.\n",
    "\n",
    "Benefits of This Prompt Chaining Approach\n",
    "Transparency and Control: By splitting the task, each step in the prompt chain can be reviewed and adjusted independently to improve specific aspects.\n",
    "Improved Performance: The model can handle each subtask effectively without being overwhelmed by a complex, detailed initial prompt.\n",
    "Adaptability: This approach can be adjusted to different inquiries by simply changing the initial extraction criteria in Prompt 1, making it versatile across various customer inquiries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9de463-0d16-4ea4-8689-cf39b5507cd5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
